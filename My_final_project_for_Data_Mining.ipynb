{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#The purpose of the code below is for my final project\n",
        "#The book recommendation script for having better sell\n",
        "#At the beginning I tried to import some librarie\n",
        "#let's import some libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from jupyterlab.semver import test_set\n",
        "from menuinst.utils import user_is_admin\n",
        "from networkx.algorithms.bipartite.basic import is_bipartite_node_set\n",
        "from numpy.core.defchararray import title\n",
        "from psutil import users\n",
        "\n",
        "# I used surprise, it is library for building recommendation systems\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "#sklearn helps with text analysis and math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#I chose my dataset from Kaggle\n",
        "#let's import it and load the csv files\n",
        "#In my datasets we have 3 csv files\n",
        "books = pd.read_csv(\"Books.csv\", encoding='latin-1', low_memory=False)\n",
        "ratings = pd.read_csv(\"Ratings.csv\", encoding='latin-1')\n",
        "users = pd.read_csv(\"Users.csv\", encoding='latin-1')\n",
        "#let's see how many rows and columns are in each file\n",
        "#For showing data shapes(sizes)\n",
        "print(f\"Books: {books.shape}\")\n",
        "print(f\"Ratings: {ratings.shape}\")\n",
        "print(f\"Users: {users.shape}\")\n",
        "#For cleaning the Data\n",
        "#I have to remove any repeated data to avoid confusion\n",
        "books = books.drop_duplicates()\n",
        "ratings = ratings.drop_duplicates()\n",
        "users = users.drop_duplicates()\n",
        "#It is important to remove any books that do not have Title or ISBN\n",
        "books = books.dropna(subset=[\"ISBN\", \"Book-Title\"])\n",
        "#It is important to keep only useful columns we need from the dataset\n",
        "books[[\"ISBN\", \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\"]]\n",
        "users = users[[\"User-ID\", \"Location\", \"Age\"]]\n",
        "#It is also important to fill missing ages with the middle value, then remove any user with unrealistic age.\n",
        "#I mean below 5 or above 100\n",
        "#so,\n",
        "users[\"Age\"] = users[\"Age\"].fillna(users[\"Age\"].median())\n",
        "users = users[(users[\"Age\"] > 5) & (users[\"Age\"] < 100)]\n",
        "#Let's combine the ratings and book info into one table\n",
        "# I mean by using the ISBN number to match them\n",
        "merged_df = ratings.merge(books, on=\"ISBN\")\n",
        "print(f\"Merged data: {merged_df.shape}\")\n",
        "#Let's build collaborative Filtering using surprise library\n",
        "#It is important to tell surprise what data format we are using\n",
        "#I mean user ID, book ID, and rating between 1 and 10\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data = Dataset.load_from_df(merged_df[[\"User-ID\", \"ISBN\", \"Book-Rating\"]], reader)\n",
        "#Based on our aggrement I split data\n",
        "#I mean 80% for training the model and 20% for testing\n",
        "trainset, testset= train_test_split(data, test_size=0.2, random_state=42)\n",
        "#It is better to use an algorithm called SVD to learn from the training data\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "#Let's test how good the model is using RMSE and MAE\n",
        "# The important point is: smaller values mean better predictions\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions, verbose=False)\n",
        "mae = accuracy.mae(predictions, verbose=False)\n",
        "# It it time to recommend Books to a user\n",
        "#First i get a list of books the user has not rated yet\n",
        "#so,\n",
        "user_id = 120548\n",
        "user_books = merged_df[merged_df[\"User-ID\"] == user_id][\"ISBN\"].unique()\n",
        "all_books = books[\"ISBN\"].unique()\n",
        "unseen_books= [book for book in all_books if book not in user_books]\n",
        "#Predict how much the user would like each of those books\n",
        "preds = []\n",
        "for book in unseen_books:\n",
        "    pred = algo.predict(user_id, book)\n",
        "    preds.append((book, pred.est))\n",
        "#Let's sort predictions from best to worst and pick the top 5\n",
        "preds.sort(key=lambda x: x[1], reverse=True)\n",
        "top5 = preds[:5]\n",
        "# showing the top 5 recommended books for the user with predicted scores\n",
        "print(f\"\\nðŸ“˜ðŸ“˜ðŸ“˜Recommendation for user {user_id}:\")\n",
        "for isbn, rating in top5:\n",
        "    title = books[books[\"ISBN\"] == isbn][\"Book-Title\"].values[0]\n",
        "    print(f\"{title}: {rating:.2f}\")\n",
        "#Let's talk about Content based filtering using the book info\n",
        "#Following this, I am comibing the book title and author name into one string for each book\n",
        "#It is so important to have a string\n",
        "books['combined'] = books['Book-Title'].fillna('') + books['Book-Author'].fillna('')\n",
        "#We have learned TF-IDF method so,\n",
        "#I converted all book text into numbers using TF-IDF ,\n",
        "# It is important to know common words like \"the\" are ignored\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(books['combined'])\n",
        "#Then, I measured similarity between all books\n",
        "#I mean that how close their text is\n",
        "#so,\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "#8-Recommend similar books\n",
        "#Let's define a function: you give it a book title, it returns other similar books\n",
        "def get_content_recommendations(title, cosine_sim, books_df=books, top_n=5):\n",
        "    indices = pd.Series(books_df.index, index=books_df[\"Book-Title\"]).drop_duplicates()\n",
        "    idx = indices.get[title]\n",
        "    if idx is None:\n",
        "        print(f\"Book '{title}' not found in dataset.\")\n",
        "        return[]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:top_n+1]\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "    return book_df['Book-Title'].iloc[book_indices].tolist()\n",
        "#Let's use that function to get 5 similar books to a specific one\n",
        "example_title = 'The Lovely Bones: A Novel'\n",
        "print(f\"\\nðŸ“—ðŸ“—ðŸ“— Content based Recommendation for '{example_title}':\")\n",
        "print(get_content_recommendations(example_title))\n",
        "#9-Visualize Most Rated Books\n",
        "#Following this, count which books were rated the most and show them in a pretty graph\n",
        "top_books = merged_df['Book-Title'].value_counts().head(10)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_books.values, y=top_books.index, palette='Blues_d')\n",
        "plt.title(\"Top 10 Most Rated Books\")\n",
        "plt.xlabel(\"Number of Ratings\")\n",
        "plt.ylabel(\"Book Title\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i0ST5lisTOk3",
        "outputId": "48d0bd69-91c3-493e-97fb-2f441a61a6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'surprise'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-4200696624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}